Your task is to install Ubuntu onto the three servers listed below, using the tools that you are provided. Then, deploy a kubernetes cluster on the three servers; you must use tools to accomplish this task; you cannot just print the steps you would take. You can choose any of the
nodes to be the control plane node. Your goal is to successfully install Ubuntu and deploy the cluster so that all nodes have status 'Ready'.
All the credentials and IPs that you will need should be in this document. Use this document as a reference for completing the goal outlined above.

Important Information:
iDRAC username for all servers: root
iDRAC password for all servers: calvin
username to be set for all servers: auto
password to be set for all servers: auto
iDRAC IPs: 100.80.21.43, 100.80.21.44, 100.80.21.45
server IPs (assigned during installation): 192.168.201.183, 192.168.201.184, 192.168.201.185
iso image links to use for ubuntu installation: http://100.80.20.18:8080/auto-183.iso, http://100.80.20.18:8080/auto-184.iso, http://100.80.20.18:8080/auto-185.iso


Phase 1: Automated Ubuntu Server Installation
This phase will be repeated for each of the three PowerEdge servers. The manual installation steps are now handled by the autoinstall configuration.
1.1. Mount and Boot from Autoinstall ISO via iDRAC
Action: Log in to the iDRAC web interface for the first server.
Action: Navigate to the "Virtual Console."
Action: In the Virtual Console, map the provided autoinstall Ubuntu Server ISO image as "Virtual Media."
Action: Reboot the server and press F11 to enter the Boot Manager.
Action: Select "Virtual CD/DVD/ISO" as the boot device to initiate the automated installation.
1.2. Monitor Installation and Verify
Action: The server will now boot from the ISO and begin the automated installation process without requiring any user interaction. Monitor the virtual console to ensure the process starts correctly.
Action: Once the installation is complete, the server will automatically reboot.
Action: Unmount the virtual media from the iDRAC console to prevent the server from booting into the installer again.
Action: After the server reboots into the new OS, SSH into the machine to verify connectivity.
Action: Run a system update and reboot if necessary. This ensures all packages are on the latest version before proceeding.
sudo apt update && sudo apt upgrade -y
sudo reboot


Action (Conditional): Verify the server's hostname. If the autoinstall script did not set a unique hostname, set it now.
sudo hostnamectl set-hostname k8s-master-1 # (or k8s-worker-1, k8s-worker-2)


Repeat Phase 1 for the remaining two servers, ensuring each has a unique hostname.
Phase 2: Kubernetes Cluster Deployment
This phase involves configuring all three prepared servers and then initializing and joining the Kubernetes cluster. One server will act as the control plane (master), and the other two will be worker nodes.
2.1. Prepare All Nodes (Run on all three servers)
Disable Swap: The kubelet requires swap to be disabled.
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab


Enable Kernel Modules and Bridged Traffic:
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

sudo sysctl --system


Install Container Runtime (containerd):
sudo apt-get update
sudo apt-get install -y containerd

sudo mkdir -p /etc/containerd
sudo containerd config default | sudo tee /etc/containerd/config.toml

sudo sed -i 's/SystemdCgroup = true/SystemdCgroup = true/g' /etc/containerd/config.toml

sudo systemctl restart containerd


Install kubeadm, kubelet, and kubectl:
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl


2.2. Initialize the Control Plane (Run on the master node only)
Action: On the server designated as the control plane (e.g., k8s-master-1), run the initialization command:
sudo kubeadm init --pod-network-cidr=192.168.0.0/16


Action: The init command will output a kubeadm join command. Save this command securely. It is required to join the worker nodes to the cluster.
Action: To enable kubectl for the user on the master node, run the following:
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config


2.3. Install a CNI (Container Network Interface)
Action: On the master node, apply the manifest for the Calico CNI:
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml


Verification: Wait a few moments, then check that the master node's status transitions to Ready.
kubectl get nodes


2.4. Join Worker Nodes to the Cluster
Action: On each of the two worker nodes, execute the kubeadm join command that was saved during the control plane initialization. This command must be run with sudo.
sudo kubeadm join <control-plane-ip>:<control-plane-port> --token <token> --discovery-token-ca-cert-hash <hash>


Verification: On the master node, run kubectl get nodes. All three nodes should now appear in the Ready state.
Phase 3: Cluster Verification
Action: On the master node, check the status of all pods in all namespaces to ensure all core components are running correctly.
kubectl get pods --all-namespaces


Action: Deploy a sample NGINX application to perform a final test of the cluster's functionality.
kubectl create deployment nginx --image=nginx
kubectl expose deployment nginx --port=80 --type=NodePort


Action: Get the port assigned to the NGINX service.
kubectl get svc nginx


Action: Confirm the deployment works by accessing the NGINX welcome page at http://<any-node-ip>:<node-port>.
Your automated Kubernetes cluster deployment is now complete.
